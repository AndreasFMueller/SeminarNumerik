%
% problemstellung.tex -- Beispiel-File für die Beschreibung des Problems
%
% (c) 2020 Prof Dr Andreas Müller, Hochschule Rapperswil
%
\section{Lernen in neuronalen Netzwerken
\label{ableitung:section:problemstellung}}
\rhead{Problemstellung}
Die Optimierung von neuronalen Netzen wird auch als {\em Lernen}
bezeichnet und funktioniert mittels Gradientenabstiegverfahren.
\index{Lernen}%
Nach jeder vollständigen Iteration über den gesamten Datensatz (Epoche) wird der Gradient neu berechnet, um den Fehler zu minimieren.
\index{Epoche}%
Die Gradientenberechnung funktioniert in den meisten neuronalen Frameworks mittels Backpropagation \cite{ableitung:backprop}.
\index{Backpropagation}%
Die Stärke des Backpropagation-Algorithmus ist, dass die Gradientenberechnung auf moderner Hardware sehr rasch vollzogen werden kann.
Gleichzeitig ist aber ein Nachteil, dass die errechnete Ableitung anfällig für Rauschen der Daten sein kann.
Aus diesem Grund ist es spannend, einen anderen Ansatz zu wählen, welcher in den kommenden Abschnitten beschrieben wird.

