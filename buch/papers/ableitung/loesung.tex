%
% loesung.tex -- Beispiel-File für die Beschreibung der Loesung
%
% (c) 2020 Prof Dr Andreas Müller, Hochschule Rapperswil
%
\section{Lösung
\label{ableitung:section:loesung}}
\rhead{Lösung}
Wie bereits erklärt, basiert die Lösung darauf, dass ein geeignetes numerisches Verfahren anstelle des Backpropagation-Algorithmus verwendet wird, um den Gradienten zu berechnen. Der Gradientenabstieg kann somit weiterhin vom Framework übernommen werden, obschon die Gradientenberechnung ersetzt wurde. Dies ermöglicht zu einem spätern Zeitpunkt einen Vergleich beider Verfahren.
\subsection{Finite Differenzen Methode}
Die Finite Differenzen Methode ist motiviert durch die klassische Ableitung. Gemäss Abbildung \ref{ableitung:fig:differential_quotient} wird die Ableitung als Sekante zweier Punkte der Funktion definiert, wobei $\Delta h \rightarrow 0$ annimmt und somit beide Punkte zu einem Punkt konvergieren. Dies entspricht dann der Tangente, welche die Ableitung am Punkt $f(x_0)$ darstellt.
Dieses Konzept ist als Differentialquotient
\begin{equation}
f'(x_0) = \lim_{{\Delta h} \rightarrow 0} \frac{f(x_0+\Delta h) - f(x_0)}{\Delta h}
\label{ableitung:equations:differentialquotient}
\end{equation}
bekannt. 
In der Numerik existiert die Möglichkeit von $\Delta h \rightarrow 0$ nicht, da die Datenpunkte diskretisiert sind. Stattdessen ist der kleinstmögliche Abstand zweier benachbarte Datenpunkte. Entsprechend kann man die numerische Ableitung (Differenzenquotient) als Quotienten
\begin{equation}
\begin{split}
f'(x_0) &= \frac{f(x_0 + h) - f(x_0)}{h} \\
f'(x_0) &= \frac{f(x_0) - f(x_0 - h)}{h} \\
f'(x_0) &= \frac{f(x_0 + h) - f(x_0 - h)}{2h} \\
\end{split}
\label{ableitung:equations:differenzenquotient}
\end{equation}
schreiben, wobei $h$ hier den Abstand zweier benachbarter Datenpunkte annimmt. Die Genauigkeit ist limitiert durch den Abstand der zwei Stützstellen.
\begin{figure}
\centering
	\begin{tikzpicture}[>=stealth',
	dot/.style={circle,draw,fill=white,inner sep=0pt,minimum size=4pt}]
	
	% draw axis lines
	\draw[->,thick] (-0.5,0) -- ++(11,0) node[below left]{$x$};
	\draw[->,thick] (0,-0.5) -- ++(0,7) node[below right]{$f(x)$};
	\coordinate (O) at (0,0);
	
	% create path for function curve
	\path[thick,red] (-0.3,2) to[out=-25, in=200] coordinate[pos=0.2] (p) coordinate[pos=0.6] (q) (9,5);
	% fill area
	\fill[blue, opacity=.1] (p) -| (q);
	% draw the secant line with fixed length
	\draw[shorten <=-1.5cm] (p) -- ($ (p)!7.5cm!(q) $) node[below right, pos=0.9]{Sekante};
	% draw function curve
	\draw[thick,red] (-0.3,2) to[out=-25, in=200] (9,5);
	
	% draw all points
	\node[dot,label={above:$P$}] (P) at (p) {};
	\node[dot,label={above:$Q$}] (Q) at (q) {};
	\node[dot] (p1) at (P |- O) {};
	\node[dot] (p2) at (Q |- O) {};
	\node[dot] (p3) at (P -| Q) {};
	
	% draw lines between nodes and place text
	\draw (P) -- node[left]{$f(x_{0})$} (p1) node[dot,label={below:$x_{0}$}]{};
	\draw (p2) node[dot,label={below:$x_{0} + \Delta h$}]{} -- (p3);
	\path (p1) -- node[below]{$\varepsilon$} (p2);
	
	% draw blue arrows between nodes
	\draw[<->,blue,thick] (P) -- node[below]{$\Delta h$} (p3);
	\draw[<->,blue,thick] (Q) -- node[right]{$f(x_{0} + \Delta h) - f(x_{0})$} (p3);
	
	% draw the explanation for the y-value of point Q
	\draw[help lines] (Q) -- (Q -| {(9.5,0)}) ++(-0.5,0) coordinate (p4);
	\draw[help lines, <->] (p4) -- node[fill=white,text=black]{$f(x_{0} + \Delta h)$} (p4 |- O);

	\end{tikzpicture}

	\caption{Differential Quotient in grafischer Darstellung}
	\label{ableitung:fig:differential_quotient}
\end{figure}

\subsection{Taylor-Methode}
Mithilfe der Taylor-Reihe 
\begin{equation}
f(x) = f(a)+{\frac {f'(a)}{1!}}(x-a)+{\frac {f''(a)}{2!}}(x-a)^{2}+{\frac {f'''(a)}{3!}}(x-a)^{3}+\cdots
\label{ableitung:eqn:taylorseries}
\end{equation}
lässt sich jede beliebige analytische Funktion $f(x)$ approximieren. Dies wird für die Herleitung einer etwas stabileren numerischen Methode zu Nutzen gemacht.
Um eine höhere Genauigkeit zu erreichen, kann die Anzahl der Stützstellen erhöhrt werden. 
Dies lässt sich mithilfe der Taylor-Reihe herleiten.
Im Grunde wird ein Polynom höherer Ordnung gesucht, welches Stützstellen in einem uniformen Abstand hat.
Die Ableitung dieses Polynoms sollte dann eine höhere Genauigkeit aufweisen, da durch die höhere Ordnung die Ursprungsfunktiuon besser um den Punkt der Ableitung approximiert werden kann.
Als Beispiel soll die erste Ableitung mit fünf Stützstellen in gleichem Abstand $h$ berechnet werden:
\begin{equation}
\frac{\delta f}{\delta x} \approx Af(x-2h) + Bf(x-h) + Cf(x) + Df(x+h) + Ef(x+2h) \text{.}
\end{equation}
Die jeweiligen Stützstellen können nun mittels Taylor-Reihe erweitert werden:
\begin{equation}
\begin{split}
Af(x-2h) \approx & Af(x) + Af'(x)(-2h) + A\frac{1}{2}f''(x)(-2h)^2+A\frac{1}{6} + f'''(x)(-2h)^3+A\frac{1}{24} + \\ 
& f''''(x)(-2h)^4 + A\frac{1}{120}f'''''(x)(-2h)^5 \\
Bf(x-h) \approx & Bf(x) + Bf'(x)(-h) + B\frac{1}{2}f''(x)(-h)^2+B\frac{1}{6} + f'''(x)(-h)^3+B\frac{1}{24} + \\ 
& f''''(x)(-h)^4 + B\frac{1}{120}f'''''(x)(-h)^5 \\
Cf(x) = & Cf(x) \\
Df(x+h) \approx & Df(x) + Df'(x)(+h) + D\frac{1}{2}f''(x)(+h)^2+D\frac{1}{6} + f'''(x)(+h)^3+D\frac{1}{24} + \\ 
& f''''(x)(+h)^4 + D\frac{1}{120}f'''''(x)(+h)^5 \\
Ef(x+2h) \approx & Ef(x) + Df'(x)(+2h) + E\frac{1}{2}f''(x)(+2h)^2+E\frac{1}{6} + f'''(x)(+2h)^3+E\frac{1}{24} + \\ 
& f''''(x)(+2h)^4 + E\frac{1}{120}f'''''(x)(+2h)^5
\end{split}
\end{equation}
Nach Einsetzen dieser Gleichungen in die erste Gleichung (siehe oben) ist ersichtlich, dass nur die Koeffizienten der 2. Ordnung bestehen bleiben müssen und folglich 1 ergeben müssen. Der Rest sollte verschwinden und 0 ergeben. Zur Unterstüzung der Lesbarkeit können die Terme nach Ableitungsgrad sortiert werden:
\begin{equation}
\begin{linsys}{6}
f(x)                        \cdot(&&  A&+&B&+&C&+&D&+&  E)&=&0 \\
\frac{1}{2} f'(x)(h)        \cdot(&&-2A&-&B& & &+&D&+& 2E)&=&\frac{2}{h}\\
\frac{1}{6} f''(x)(h^2)     \cdot(&&14A&+&B& & &+&D&+& 4E)&=&0 \\
\frac{1}{24} f''''(x)(h^3)  \cdot(&&-8A&-&B& & &+&D&+& 8E)&=&0 \\
\frac{1}{120} f'''''(x)(h^4)\cdot(&&16A&+&B& & &+&D&+&16E)&=&0 
\end{linsys}
\end{equation}
%
%\begin{equation}
%\begin{split}
%f(x) \cdot (A + B + C + D + E) & = 0 \\
%\frac{1}{2} f'(x)(h) \cdot (-2A - B + D + 2E) &= \frac{2}{h}\\
%\frac{1}{6} f''(x)(h^2) \cdot (4A + B + D + 4E) &= 0 \\
%\frac{1}{24} f''''(x)(h^3) \cdot (-8A - B + D + 8E) &= 0 \\
%\frac{1}{120} f'''''(x)(h^4) \cdot (16A + B + D + 16E) &= 0 
%\end{split}
%\end{equation}
Dies lässt sich auch in Matrix Form wie folgt schreiben:
\begin{align}
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 \\
-2 & -1 & 0 & 1 & 2 \\
4 & 1 & 0 & 1 & 4 \\
-8 & -1 & 0 & 1 & 8 \\
16 & 1 & 0 & 1 & 16 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
A \\
B \\
C \\
D \\
E \\
\end{bmatrix}
= \frac{1}{h} 
\begin{bmatrix}
0 \\
2 \\
0 \\
0 \\
0 \\
\end{bmatrix}
\end{align}
Diese Matrix lässt sich nach
\begin{align}
\begin{bmatrix}
A \\
B \\
C \\
D \\
E \\
\end{bmatrix}
=
\frac{1}{h}
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 \\
-2 & -1 & 0 & 1 & 2 \\
4 & 1 & 0 & 1 & 4 \\
-8 & -1 & 0 & 1 & 8 \\
16 & 1 & 0 & 1 & 16 \\
\end{bmatrix}^{-1}
\begin{bmatrix}
0 \\
2 \\
0 \\
0 \\
0 \\
\end{bmatrix}
=
\frac{1}{h}
\begin{bmatrix}
\frac{1}{12} \\
\frac{-2}{3} \\
0 \\
\frac{2}{3} \\
\frac{1}{12} \\
\end{bmatrix}
\end{align}
invertieren.
Dies ergibt die erste Ableitung
\begin{align}
f'(x)  \approx \frac{1f(x-2h) - 8f(x-h) + 8f(x-h) - 1f(x+2h)}{12h}
\label{ableitung:eqn:aprox_5}
\end{align}
angenähert durch fünf Stützstellen.
Analog kann dies natürlich für höhere Ableitungen oder non-uniforme Stützstellen gemacht werden.
\subsubsection{Fehlerabschätzung}
Zur Berechnung der ersten Ableitung mit fünf Stützstellen werden die ersten fünf Terme der Taylor-Reihe verwendet. Die Ungenauigkeit dieser Methode ist somit abhängig vom Restterm
\begin{align}
\frac{f(x-2h) - 8f(x-h) + 8f(x-h) - f(x+2h)}{12h} = f'(x) - \frac{1}{30} f^{(5)} (x)h^{4}+R_n(x) \text{.}
\label{ableitung:eqn:error}
\end{align}
Der Restterm $R_{n}$ ist abhängig von der Taylor-Reihe der zu ableitenden Funktion. Funktionen lassen sich somit mit mehr Stützstellen genauer approximieren, da der Rest $R_n$ mit steigender Taylor-Ordnung kleiner wird. Wenn aber die Terme höherer Ordnung schneller anwachsen als die Fakultät
\begin{align}
	f^{(n)} > n! \text{,}
	\label{eqn:fakult}
\end{align}
so ist die Approximation nicht genau.
Aus der Fehlerabschätzung ergeben sich grundsätzlich zwei Typen an Problemen, die interessant sind zu untersuchen.
Die Terme des ersten Types werden mit zunehmender Ordnung kleiner, während bei dem zweiten Typ die Terme immer grösser werden.
Gleichzeitig muss aber ein guter Kompromiss gefunden werden, welcher den Abstand $h$ nicht zu klein werden lässt. Ein sehr kleines $h$ führt zu mehr Rauschanfälligkeit, da bei benachbarten Punkten ein Rauschen höher ins Gewicht fällt.

Als Beispiel für eine Funktion, bei welcher die Terme kleiner werden, können die trigonometrischen Funktionen dienen. Im Speziellen hat der Sinus
\begin{equation}
	\sin = \sum _{n=0}^{\infty }{\frac {(-1)^{n}}{(2n+1)!}}x^{2n+1} \left(x\right)\approx x-{\frac {x^{3}}{3!}}+{\frac {x^{5}}{5!}}-{\frac {x^{7}}{7!}}+{\frac{x^9}{9!}}+R_n
\end{equation}
eine alternierende Taylor-Reihe. Der Fehler
\begin{equation}
R_n < \biggl| {\frac{x^{11}}{11!} \biggr|}
\end{equation}
kann somit auf einem Interval von $-1<x<1$ nicht grösser werden als $\approx 2.5 \cdot 10^{-8}$.
Auf der anderen Seite erweist sich die die geometrische Reihe
\begin{equation}
	\frac{1}{1-x} = \sum _{n=0}^{\infty} 1 + x + x^2 + x^3 + x^4 + R_n
\end{equation}
als sehr schwer zu approximieren. Der Fehler $R_n$ läuft ab $x>1$ nach $\infty$.
Für die Experimente werden aus diesem Grund verschiedene Abstände $h$ und eine unterschiedliche Anzahl Stützstellen verwendet, um ein Optimum für den jeweiligen Fall zu finden. Es ist jedoch zu erwarten, dass mit steigender Anzahl an Stützstellen das neuronale Netzwerk schneller konvergiert, da die einzelnen Gradienten weniger Rauschen aufweisen.