\subsection{Was ist Lernen?}
Neuronale Netzwerke lernen durch den Vergleich einer errechneten Ausgabe mit der erwarteten Ausgabe. Sie gehören in der Regel zur Gruppe der 'supervised learning'-Algorithmen. Diese Gruppe von Algorithmen benötigen einen Datensatz, welcher aus einem Input $x$ und einem erwarteten Output $y$ bestehet.
\begin{figure}
	\begin{center}
		\begin{tikzpicture}[auto, node distance=2cm,>=latex']
		% We start by placing the blocks
		\node [input, name=input] {};
		\node [block, right of=input] (neuronalnet) {$N(x)$};
		\node [block, right of=neuronalnet,
		node distance=3cm] (loss) {$\Lb (y, \hat{y})$};
		% We draw an edge between the controller and system block to 
		% calculate the coordinate u. We need it to place the measurement block. 
		\draw [->] (neuronalnet) -> node[name=u] {$\hat{y}$} (loss);
		\node [output, right of=loss] (output) {};
		
		% Once the nodes are placed, connecting them is easy. 
		\draw [draw,->] (input) -- node {$x$} (neuronalnet);
		\draw [draw,<-] (loss) -- node {$y$} (output);
		\draw [out=-90, in=-90, draw, ->] (loss) to node[below] {Update} (neuronalnet);

		\end{tikzpicture}
	
		\caption{Trainieren eines neuronalen Netzwerks}
		\label{ableitung:fig:nn_concept}
	\end{center}
\end{figure}
Das neuronale Netzwerk als Blackbox versucht aus dem Input $x$ einen brauchbaren Output $\hat{y}$ zu erzeugen, welcher dann mittels einer Fehlerfunktion $\Lb$ zum bestehnden Output $y$ verglichen wird. Der Lernprozess kann Schematisch wie in Abbildung \ref{ableitung:fig:nn_concept} dargestellt werden. Nach jeder Trainingsiteration werden die inneren Parameter des neuronalen Netzwerks $N(x)$ optimiert, um das bestmögliche Resultat für die gegebenen Datenpaare $(x,y)$ zu errechnen.
Die Bewertung des aktuellen Lernstands ist mittels der Fehlerfunktion
\begin{equation}
	\Lb(y_{i},\hat{y_i})
\end{equation}
gegeben. Dabei gilt, dass 
\begin{equation}
\hat{y} = N(x)
\end{equation}
dem berechneten Output des neuronalen Netzwerks $N(x)$ entspricht. Dies lässt sich in die Fehlerfunktion (Mean-Squared-Error)
\begin{equation}
\Lb(y,N(x))
\end{equation}
einsetzten.
Die Fehlerfunktion $\Lb$ liefert somit ein Mass für den Fehler, also eine quantitative Bewertung des Fehlers und wird in der Literatur auch oft \textit{Loss}-Funktion genannt. Als einfaches Beispiel für eine solche Fehlerfunktion $\Lb$ kann die mittlere quadratische Abweichung 
\begin{equation}
\text{MSE: } \Lb (y, \hat{y}) = \frac{1}{2} \sum_i \left( \hat{y} - y \right)^2
\label{ableitung:eqn:loss}
\end{equation}
verwendet werden. Diese hat die Eigenschaft, dass sie Vorzeichen unabhänig ist und bei grösserer Distanz zwischen $y$ und $\hat{y}$ der Fehler auch dementsprechend grösser wird.
Die Aufgabe des Lernprozesses ist es nun, die bestmögliche Lösung für den Fehler zu finden. Entsprechen gilt es für
\begin{equation}
	\min \left( \Lb(y,\hat{y}) \right) 
	\label{ableitung:eqn:min_loss}
\end{equation}
eine optimale Lösung zu finden. Da $x$ und $y$ durch den Datensatz gegeben sind, muss nun auf den inneren Parameterraum des neuronalen Netzwerks eingegangen werden.

