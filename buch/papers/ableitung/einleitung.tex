%
% einleitung.tex -- Beispiel-File für die Einleitung
%
% (c) 2020 Prof Dr Andreas Müller, Hochschule Rapperswil
%
\section{Einleitung\label{ableitung:section:einleitung}}
\rhead{Einleitung}
Dieses Kapitel befasst sich mit der numerischen Ableitung, im Speziellen mit der Finite Differenzen Methode (FDM).
Bei dieser Methode handelt es sich um ein Verfahren zur Bestimmung der Ableitung. Die Methode war schon Gauss und Boltzmann bekannt, war aber bis in die 1940er Jahre nicht sehr verbreitet, um angewandte Probleme zu lösen. Das Verfahren ist mathematisch sehr einfach zu erklären und ist ebenfalls einfach umzusetzen. Die Einfachheit der Methode lässt Raum, um ein etwas schwierigeres Beispiel zu verwenden, da die Umsetzung in Code unkompliziert ist.

Als Einführung der Methode wird aus diesem Grunde der Gradient im neuronalen Netzwerk berechnet, welcher eine zentrale Rolle im Lernprozess hat.
Neuronale Netzwerke sind durch die kostengünstig verfügbare Computerleistung in den letzten Jahrzehnten sehr populär geworden. Moderne Forschung setzt den Schwerpunkt oft auf den angewandten Bereich des Gebiets. Im Fokus ist eine Vielzahl von Problemen, meistens im Bereich der Bild-, Signal- oder Mustererkennung, aber auch Sprachsynthese und Übersetzung können von neuronalen Netzwerken profitieren. Die funamentalen und mathematischen Grundlagen dieses Gebiets werden heute teils weniger stark hinterfragt, da ein sehr hoher Abstraktionsgrad die Mathematik versteckt.

Die starke Popularität und die günstige Computerleistung resultiert in immer grösseren Netzwerken und komplizierteren Strukturen. Die immer grösser werdenden Netzwerke sind durch ihre wachsende Tiefe auch anfälliger für numerische Probleme. Deshalb liegt der Fokus dieser Arbeit auf einfacher Mathematik, die aufzeigt, wo Probleme während des Trainings von sehr tiefen neuronalen Netzwerken entstehen können.