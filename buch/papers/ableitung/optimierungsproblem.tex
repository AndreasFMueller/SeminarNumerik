\subsection{Das Optimierungsproblem}
Wie bereits erwähnt, ist durch die extrem hohe Anzahl an Parametern ein Durchprobieren aller Parameterkombinationen nicht möglich.
Es würde kein Resultat in nützlicher Frist gefunden werden können.
Aus diesem Grund werden andere Verfahren verwendet, um das neuronale Netzwerk zu optimieren.
Alle diese Verfahren basieren auf dem Gradientenabstieg und werden in der Literatur oft auch 'Optimizer' genannt. Näheres zum Gradientenabstieg kann in Kapitel \textbf{xyz} gefunden werden.
Ein einfacher Algorithmus, welcher zur Optimierung neuronaler Netzwerke verwendet wird, ist der sogenante stochastische gradienten Abstieg.
Der Algorithmus multipliziert die aktuellen Gewichte mit dem Gradienten und einer bestimmten Schrittweite $\nu$. 
Der Parameter $\nu$ wird oft auch als 'learning rate' bezeichnet.
Der Algorithmus bricht erst ab, wenn ein Minima erreicht ist oder die Anzahl an Iterationen ausgeschöpft ist.
Das Anpassen der inneren Parameter $p$ 
\begin{equation}
p:=p-\eta \nabla \Lb_{i}(p)
\end{equation}
lässt sich für alle Gewichte $w$ und Bias-Parameter $b$ festhalten.
Die Fehlerfunktion $Lb$
\begin{equation}
\Lb(p)=\sum_{i=1}^{n}\Lb_{i}(p)=\sum_{i=1}^{n}\Lb(\hat{y_i}-y_i)^2
\end{equation}
berechnet den Fehler zwischen dem Output $y$ und dem erwarteten Output $\hat{y}$. Aus Gründen der Einfachheit wird hier das vollständiges Einsetzten des Ausdrücks für das neuronale Netzwerk aus \eqref{ableitung:eqn:full_net} weggelassen.

Als nächstes wird ein einfaches neuronales Netz benötigt, welche aus einem Input $x$ einem erwarteten Output $y$ und den inneren Parametern $w$ und $b$ besteht.
\begin{equation}
	\hat{y} = \sigma \left( w_1 \cdot x + b_1 \right)
\end{equation}
Dieser Ausdruck kann eingesetzt werden in die Fehlerfunktion $\Lb$. Die anschliessende Differentiation nach den inneren Parametern $w$ und $b$ führt zum Gradiente der beiden Parametern. Dieser Ausdruck kann nun verwendet um die zwei Parameter nach der Iteration anzupassen. Ausgeschrieben sieht man nun
\begin{equation}
\begin{bmatrix}w_{1}\\b_{1}\end{bmatrix}:={\begin{bmatrix}w_{1}\\b_{1}\end{bmatrix}}-\eta {\begin{bmatrix}
{\frac {\partial }{\partial w_{1}}}(\sigma(w_{1}x_{i}+b_{1})-y_{i})^{2}\\
{\frac {\partial }{\partial b_{1}}}(\sigma(w_{1}x_{i}+b_{1})-y_{i})^{2}\end{bmatrix}}=
{\begin{bmatrix}w_{1}\\b_{1}\end{bmatrix}}-\eta {\begin{bmatrix}2((\sigma(w_{1}x_{i}+b_{1}) - y_i) \cdot \sigma(w_{1}x_{i}+b_{1} \cdot (w_{1}x_{i}+b_{1}) x_{i} \\2((\sigma(w_{1}x_{i}+b_{1}) - y_i) \cdot \sigma(w_{1}x_{i}+b_{1} \cdot (w_{1}x_{i}+b_{1}) \end{bmatrix}}
\end{equation}
als Ausdruck des Updates der inneren Parametern in jeder Iteration des Lernprozesses.

Es ist unschwer zu erkennen, dass mit steigender Netzkomplexität die Terme noch viel grösser werden. Dies ist auf die Kettenregel $u'(v(x)) = u'(v(x)) \cdot (v'(x))$ zurückzuführen. Mit diesem Verfahren ist der Gradientenabstieg zwar möglich, jedoch sehr rechenintensiv. Aus diesem Grunde verwenden die meisten Frameworks den Backpropagation Algorithmus, welcher etwas effizienter ist.