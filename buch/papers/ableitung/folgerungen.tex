%
% problemstellung.tex -- Beispiel-File für die Beschreibung des Problems
%
% (c) 2020 Prof Dr Andreas Müller, Hochschule Rapperswil
%
\section{Folgerungen
\label{ableitung:section:folgerungen}}
\rhead{Folgerungen}
Das Gedankenexperiment, welches in dieser Arbeit beschrieben wurde, hat wie in den Resultaten ersichtlich ist, tatsächlich funktioniert. Es konnte an einem einfachen Netzwerk gezeigt werden, dass eine eigene Implementation für die Berechnung des Gradienten funktioniert. Im direkten Vergleich zwischen dem FDM- und dem Backpropagation-Algorithmus, hat ersterer früher zu einem kleineren Fehler konvergiert. Vorausgesetzt, dass die Parameter gut gewählt wurden. Nach der gleichen Anzahl an Iterationen lag der Fehler der FDM-Implementation um einen Faktor $10^3$ niedriger. Dabei spielt die Anzahl an Stützstellen eine wichtigere Rolle, als die Wahl des Abstandes $h$. Ohne an dieser Stelle zu stark im Detail auf die Fehlerrechnung einzugehen, kann mit einfacher Intuition dieses Phänomen erklärt werden.

Die meisten Aktivierungsfunktionen im neuronalen Netzwerk haben einen Abbildungsbereich zwischen $[-1, 1]$ oder $[0, 1]$. Die Kettenregel hat zur Folge, dass für die Berechnung des Fehlers in den vorderen Schichten diese kleine Zahlen oft miteinander multipliziert werden müssen. Der Fehler in der Gradientenberechnung nimmt zu, da das numerische Rauschen durch die Gleitkomma-Darstellung von Schicht zu Schicht grösser wird. Dieses Phänomen wird auch als \textit{Vanishing Gradient Problem} bezeichnet und ist ein bekanntes Problem, welches bei Netzwerken mit hoher Tiefe besonders stark ausgeprägt ist. Durch den Verzicht der Rückwärtsberechnung mittels Kettenregel entfällt dieser Fehler bei der FDM vollständig. 

Weiter kommt eine höhere Anzahl an Stützstellen zu nutze, die gewisse Toleranzen in der Gleitkomma-Darstellung weniger stark ins Gewicht fallen lässt. Dies kann wie folgt erklärt werden: Das Zeichnen einer Parabel durch drei Punkte mit gewisser Unschärfe ist deutlich ungenauer, als das Zeichnen einer Parabel durch sieben Punkte mit ähnlicher Unschärfe. Als Unschärfe ist die Genauigkeit der Position der Punkte gemeint, durch welche die gezeichnete Parabel verlaufen soll.

Anhand eines Experiments wurde gezeigt, dass die FDM durchaus Potential zum Trainieren von tiefen neuronalen Netzwerken hat. An diesem Punkt wäre es ausserordentlich spannend gewesen tiefere Netzwerke zu trainieren. An einem etwas älteren aber gut dokumentierten Netzwerk (LeCun Net5) sollte dies probiert werden. Das Netzwerk wurde in den achziger Jahren verwendet um Handschriften auf dem bekannten MNIST-Datensatz zu erkennen. Es bietet eine gute Grundlage um qualitative Aussagen zu machen. An dieser Stelle ist zu erwähnen, dass das Training mittels Backpropagation Algorithmus in wenigen Minuten vollzogen war, während das Training mittels FDM nach mehreren Tagen noch nicht genügend Iterationen durchlaufen hat um eine abschliessende qualitative Aussage zu machen. Es war nie Ziel, dieses Projektes den Algorithmus soweit zu optimieren, dass ein Vergleich einer solchen Struktur möglich gewesen wäre. Da die Laufzeit um einen sehr grossen Faktor sich unterschieden hat, wurde auf eine Optimierung verzichtet. 