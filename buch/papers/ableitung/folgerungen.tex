%
% problemstellung.tex -- Beispiel-File für die Beschreibung des Problems
%
% (c) 2020 Prof Dr Andreas Müller, Hochschule Rapperswil
%
\section{Folgerungen
\label{ableitung:section:folgerungen}}
\rhead{Folgerungen}
Die Resultate zeigen anhand eines einfachen Netzwerks, dass die hier
vorgestellte Implementation für die Berechnung des Gradienten funktioniert.
Im direkten Vergleich zwischen dem FDM- und dem
Backpropagation-Algorithmus, hat ersterer früher zu einem kleineren
Fehler konvergiert, unter der Voraussetzung,
dass die Parameter gut gewählt wurden.
Nach der gleichen Anzahl an Iterationen lag der Fehler der FDM-Implementation um einen Faktor $10^3$ niedriger.
Dabei spielt die Anzahl an Stützstellen eine wichtigere Rolle, als die Wahl des Abstandes $h$.
Ohne an dieser Stelle zu stark im Detail auf die Fehlerrechnung einzugehen, kann die folgende Intuition dieses Phänomen erklären.

Die am häufigsten verwendeten Aktivierungsfunktionen in neuronalen
Netzwerken haben Wertebereich $[-1, 1]$ oder $[0, 1]$.
Die Kettenregel hat zur Folge, dass für die Berechnung des Fehlers
in den vorderen Schichten diese kleine Zahlen oft miteinander
multipliziert werden müssen.
\index{Kettenregel}%
Der Fehler in der Gradientenberechnung nimmt zu, da das numerische
Rauschen durch die Gleitkommadarstellung von Schicht zu Schicht
grösser wird.
Dieses Phänomen wird auch als {\em vanishing gradient problem}
bezeichnet und ist ein bekanntes Problem, welches bei Netzwerken
mit hoher Tiefe besonders stark ausgeprägt ist.
\index{vanishing gradient problem}%
Durch den Verzicht der Rückwärtsberechnung mittels Kettenregel entfällt
dieser Fehler bei der FDM vollständig. 

Weiter verbessert eine höhere Anzahl an Stütztstellen die Genauigkeit des
Gradienten, da gewisse Toleranzen in der Gleitkommadarstellung weniger
stark ins Gewicht fällt.
Dies kann wie folgt erklärt werden: Das Zeichnen einer Parabel durch drei Punkte mit gewisser Unschärfe ist deutlich ungenauer, als das Zeichnen einer Parabel durch sieben Punkte mit ähnlicher Unschärfe.
Als Unschärfe ist die Genauigkeit der Position der Punkte gemeint, durch welche die gezeichnete Parabel verlaufen soll.

Das Experiment hat gezeigt, dass die FDM durchaus Potential zum Trainieren
von tiefen neuronalen Netzwerken hat.
An diesem Punkt wäre es ausserordentlich spannend gewesen tiefere Netzwerke zu trainieren.
An einem etwas älteren aber gut dokumentierten Netzwerk (LeCun Net5)
sollte dies probiert werden.
Das Netzwerk wurde in den achziger Jahren verwendet um Handschriften
auf dem bekannten MNIST-Datensatz zu erkennen.
\index{MNIST-Datensatz}%
Es bietet eine gute Grundlage um qualitative Aussagen zu machen.

Die erfreuliche Genauigkeit, die mit der FDM erreicht werden konnte,
hat aber auch ihren Preis.
Das Training mittels Backpropagation Algorithmus war in wenigen
Minuten beendet, während das Training mittels FDM nach mehreren
Tagen noch nicht genügend Iterationen durchlaufen hat um eine
abschliessende qualitative Aussage zu machen.
Es war nie Ziel dieses Projektes den Algorithmus soweit zu optimieren,
dass ein Vergleich einer solchen Struktur möglich gewesen wäre.
Da die Laufzeit um einen sehr grossen Faktor sich unterschieden
hat, wurde auf eine Optimierung verzichtet.
