\subsection{Der Backpropagation Algorithmus}
Der Backpropagation Algorithmus kann als einfache Matrixmultiplikation
\begin{equation}
\Lb(y_{i}, \sigma^{l}(W^{l}\sigma^{l-1}(W^{l-1}\cdots \sigma^{1}(W^{1}x+b^{1}) + b^{l-1} \cdots ) )
\end{equation}
verstanden werden. Sofern die Verlustfunktion $\Lb$ einen Skalar als Resultat liefert und die einzelnen Schichten $l$ des Netzwerks jeweils nur mit ihrer nachfolgenden Schicht $l+1$ verbunden sind.

Daraus resultiert, dass die partiellen Ableitungen  $\partial \Lb / \partial w^l_{jk}$ und $\partial \Lb / \partial b^l_j$ berechnet werden müssen. Das Interesse dieser partiellen Ableitung liegt eigentlich auf der Fehleränderung $\delta$ in jedem Neuron. $\delta^{l}_{j}$ ist somit die Änderung des Fehlers im Neuron $j$ in der Schicht $l$. Somit ist Backpropagation ein Algorithmus um $\delta^{l}_{j}$ zu berechnen.  welchen wir auf $\partial \Lb / \partial w^l_{jk}$ und $\partial \Lb / \partial b^l_j$ zurück führen können. Für die letzte Schicht
\begin{equation}
\delta^{L}_{j} = \frac{\partial \Lb}{\partial a^{L}_{j}} \sigma'(z^L_j)
\end{equation}
gilt (Index $l=\textbf{last}$). Analog dazu in Matrixschreibweise
\begin{equation}
\delta^{L} = \nabla_a \Lb \odot \sigma'(z^L)
\end{equation}
mit $\odot$ als komponentenweise Multiplikation der Matrix (Hadamard Produkt). Es kann für die Fehlerfunktion $\Lb$ eine beliebige Funktion verwendet werden. Oft wird der $L_1$ oder $L_2$ Fehler verwendet. Für dieses Beispiel verwenden wir die quadatische Fehlerfunktion ($L_2$) auch bekannt als Mean-Square-Error.
\begin{equation}
\Lb = \frac{1}{2} \sum_j \left( \hat{y} - y \right)^2
\end{equation}
Somit resultiert nach der partiellen Ableitung von $\Lb$ und einsetzten von $a^L$ die Gleichung:
\begin{equation}
\delta^{L} = (\hat{y} - a^L) \odot \sigma'(z^L)
\end{equation}
Mit diesen Annahmen wurde nun die Änderung des Fehlers abhänig von den Parametern der letzten Schicht berechnet. Der nächste Schritt wird sein herauszufinden, wie dieser durch das Netzwerk rückwärts gerechnet werden kann --- back propagiert.
\begin{equation}
\delta^{l} = ((w^{l+1} \delta{l+1}) \cdot \sigma'(z^l)
\end{equation}
Die Transponierte der Matrix $w^{l+1}$ kann man sich intuitiv vorstellen, als das 'Rückwärtsrechnen' des gewichteten Fehlers, da $w^{l+1} \cdot (w^{l+1})^T = I$. Dies ermöglicht die Änderung des Fehlers pro Neuron in der vorherliegenden Schicht zu bestimmen. Abschliesend müssen noch die Gradienten
\begin{equation}
\begin{split}
\frac{\partial \Lb}{\partial b} & = \delta \\
\frac{\partial \Lb}{\partial w} & = a^{l-1}_{k} \delta^{l}_j = a_{\text{in}} \delta_{\text{out}}
\end{split}
\end{equation}
berechnet werden.