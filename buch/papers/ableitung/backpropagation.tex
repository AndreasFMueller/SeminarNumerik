\subsection{Der Backpropagation Algorithmus}
Der Backpropagation Algorithmus kann als einfache Matrixmultiplikation
\begin{equation}
\Lb(y_{i}, \sigma^{l}(W^{l}\sigma^{l-1}(W^{l-1}\cdots \sigma^{1}(W^{1}x_i+b^{1}) + b^{l-1} \cdots ) )
\end{equation}
verstanden werden, wobei $W^{n}$ und $b^{n}$ jeweils die Gewichte im $n$-ten Layer des neuronalen Netzwerks darstellen. Dies gilt unter der Annahme, dass die Fehlerfunktion $\Lb$ einen Skalar als Resultat liefert und die einzelnen Schichten $l$ jeweils nur mit ihrer benachbarten Schicht $l+1$ verbunden sind. Ziel des Trainings ist es, für den Trainingsdatensatz, ein geeignetes Parameterset für $w$ und $b$ zu finden, so dass der resultierende Fehler aus $\Lb(\hat{y}, y)$ möglichst klein wird. Für den Gradientenabstieg nach jeder Iteration ist somit die Änderung des Fehlers in Abhänigkeit der inneren Parametern relevant. Dies kann durch die partielle Ableitung $\partial \Lb / \partial w^l_{jk}$ und $\partial \Lb / \partial b^l_j$ ausgedrückt werden. Die partiellen Ableitungen kann man als 
\begin{equation}
\delta^{l}_{j} = \frac{\partial \Lb}{\partial a^{l}_{j}} \sigma'(z^l_j)
\end{equation}
ausrücken, was dem Änderung des Fehlers im Neuron $j$ der Schicht $l$ entspricht. Die letzte Schicht mit dem Index $L$ als 'last', kann in Matrixschreibweise
\begin{equation}
\delta^{L} = \nabla_a \Lb \odot \sigma'(z^L)
\end{equation}
geschrieben werden, wobei $\odot$ der komponentenweise Multiplikation der Matrix (Hadamard Produkt) entspricht. Um den Fehler nun zu berechnen legen wir für die Funktion $\Lb$ die quadatische Fehlerfunktion ($L_2$), auch bekannt als Mean-Square-Error, fest
\begin{equation}
\Lb = \frac{1}{2} \sum_j \left( \hat{y} - y \right)^2 \text{.}
\end{equation}
Somit resultiert nach der partiellen Ableitung von $\Lb$ und einsetzten von $a^L$ die Gleichung:
\begin{equation}
\delta^{L} = (\hat{y} - a^L) \odot \sigma'(z^L)
\end{equation}
Mit diesen Annahmen wurde nun die Änderung des Fehlers abhängig von den Parametern der letzten Schicht berechnet. Der nächste Schritt wird sein herauszufinden, wie dieser durch das Netzwerk rückwärts gerechnet werden kann --- back propagiert.
\begin{equation}
\delta^{l} = ((w^{l+1} \delta^{l+1})^T \cdot \sigma'(z^l)
\end{equation}
Die Transponierte der Matrix $w^{l+1}$ kann man sich intuitiv vorstellen, als das 'Rückwärtsrechnen' des gewichteten Fehlers, da $w^{l+1} \cdot (w^{l+1})^T = I$, wobei $I$ für die Einheitsmatrix steht. Dies ermöglicht die Änderung des Fehlers pro Neuron in der vorherliegenden Schicht zu bestimmen. Abschliesend müssen noch die Gradienten
\begin{equation}
\begin{split}
\frac{\partial \Lb}{\partial b} & = \delta \\
\frac{\partial \Lb}{\partial w} & = a^{l-1}_{k} \delta^{l}_j = a_{\text{in}} \delta_{\text{out}}
\end{split}
\end{equation}
berechnet werden. Das Resultat wird verwendet um mittels Gradientenabstieg die neuen inneren Parameter $w_\text{new}$ und $b_\text{new}$ zu bestimmen.