\subsection{Der Backpropagation-Algorithmus}

%5. 25.2.4 Backprop: Dieser Abschnitt ist immer noch sehr schwer verständlich. Im Moment ist für mich der grösste Stolperstein, dass Sie nicht erklären, was z eigentlich ist. Auch ist der erste Absatz von 25.2.4 über eine halbe Seite lang, da braucht man einen ziemlich langen Atem, das durchzustehen. Oft ist das auch ein Hinweis darauf, dass man die Ideen noch nicht genügend aufgeschlüsselt hat. Vielleicht sollten Sie den Absatz unterteilen:
%
%1. Teil/Absatz: Das Problem klären. Gesicht ist eine Parameter-Änderung, die eine Output-Änderung hervorruft, die den Fehler reduziert. Statt den Fehler in einem Schritt zu reduzieren, reduziert man ihn Schrittweise, indem man in jedem Layer versucht, die Parameter so zu ändern, dass der Fehler reduziert wird. Dies führt auf einen Fehler in der vorangegangenen Schicht, den man im nächsten Schritt reduzieren will.
%
%2. Teil/Absatz: Bezeichnungen. Da man jetzt weiss, was der Plan ist, kann man sagen, wie man diese Fehler und Fehleränderungen bezeichnen will. Damit ist dann klar, was \delta und z bedeuten.
%
%3. Teil/Absatz: Berechnung der Änderung in jedem Layer. Hier kann man jetzt die partiellen Ableitungen und das Hadamard-Produkt bringen.
%
%4. Teil/Absatz: Backprop, da haben Sie ja schon einen eigenen Absatz.

Der Backpropagation-Algorithmus kann als einfache Matrixmultiplikation
\begin{equation}
\Lb(y_{i}, \sigma^{l}(W^{l}\sigma^{l-1}(W^{l-1}\cdots \sigma^{1}(W^{1}x_i+b^{1}) + b^{l-1} \cdots ) )
\end{equation}
verstanden werden, wobei $W^{n}$ und $b^{n}$ jeweils die Gewichte im $n$-ten Layer des neuronalen Netzwerks darstellen. Dies gilt unter der Annahme, dass die Fehlerfunktion $\Lb$ einen Skalar als Resultat liefert und die einzelnen Schichten $l$ jeweils nur mit ihrer benachbarten Schicht $l+1$ verbunden sind. Ziel des Trainings ist, für den Trainingsdatensatz, ein geeignetes Parametersatz für $w$ und $b$ zu finden, so dass der resultierende Fehler aus $\Lb(\hat{y}, y)$ möglichst klein wird. Für den Gradientenabstieg nach jeder Iteration ist somit die Änderung des Fehlers in Abhänigkeit der inneren Parametern relevant. Dies kann durch die partielle Ableitung $\partial \Lb / \partial w^l_{jk}$ und $\partial \Lb / \partial b^l_j$ ausgedrückt werden. Die partiellen Ableitungen kann man als 
\begin{equation}
\delta^{l}_{j} = \frac{\partial \Lb}{\partial a^{l}_{j}} \sigma'(z^l_j)
\end{equation}
ausdrücken, was einer Änderung des Fehlers im Neuron $j$ der Schicht $l$ entspricht. Die letzte Schicht mit dem Index $L$ als 'last', kann in Matrixschreibweise
\begin{equation}
\delta^{L} = \nabla_a \Lb \odot \sigma'(z^L)
\end{equation}
geschrieben werden, wobei $\odot$ der komponentenweise Multiplikation der Matrix (Hadamard Produkt) entspricht. Um den Fehler zu berechnen, legen wir für die Funktion $\Lb$ die quadratische Fehlerfunktion ($L_2$), auch bekannt als Mean-Square-Error, fest
\begin{equation}
\Lb = \frac{1}{2} \sum_j \left( \hat{y} - y \right)^2 \text{.}
\end{equation}
Somit resultiert nach der partiellen Ableitung von $\Lb$ und einsetzten von $a^L$ die Gleichung:
\begin{equation}
\delta^{L} = (\hat{y} - a^L) \odot \sigma'(z^L)
\end{equation}
Mit diesen Annahmen wurde nun die Änderung des Fehlers abhängig von den Parametern der letzten Schicht berechnet. Die Matrixschreibweise erlaubt es für den Computer sehr performanten Code zu schreiben, aus diesem Grund ist dieser Algorithmus so populär. 

Als nächstes muss nun rückwärts gerechnet also --- back propagiert --- werden. Die Änderung des Fehlers
\begin{equation}
\delta^{l} = ((w^{l+1} \delta^{l+1})^T \odot \sigma'(z^l)
\end{equation}
beschreibt nun den Zusammenhang zwischen den Schichten $l+1$ und $l$, was an den Ausdrücken $\delta^{l+1}$ und $ \delta^{l}$ abzulesen ist. Die Transponierte der Matrix $w^{l+1}$ kann man sich intuitiv vorstellen, als das 'Rückwärtsrechnen' des gewichteten Fehlers, da $w^{l+1} \cdot (w^{l+1})^T = I$, wobei $I$ für die Einheitsmatrix steht. Dies ermöglicht die Änderung des Fehlers pro Neuron in der vorherliegenden Schicht zu bestimmen. Abschliesend müssen noch die Gradienten
\begin{equation}
\begin{split}
\frac{\partial \Lb}{\partial b} & = \delta \\
\frac{\partial \Lb}{\partial w} & = a^{l-1}_{k} \delta^{l}_j = a_{\text{in}} \delta_{\text{out}}
\end{split}
\end{equation}
berechnet werden. Das Resultat wird verwendet um mittels Gradientenabstieg die neuen inneren Parameter $w_\text{new}$ und $b_\text{new}$ zu bestimmen.