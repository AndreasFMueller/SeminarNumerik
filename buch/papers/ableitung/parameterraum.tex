\subsection{Der Parameterraum}
Das neuronale Netzwerk wird durch eine sehr hohe Anzahl an Parametern charakterisiert, welche im durch den Lernprozesses optimiert werden müssen. Um zu verstehen, woher diese Parameter stammen, muss der innere Aufbau eines neuronalen Netzwerks betrachtet werden.
\begin{figure}
	\begin{center}
		\input{papers/ableitung/neuronal_network.tex}
		\caption{Übersicht eines Neuronalen Netzwerks}
		\label{ableitung:fig:neuronal_network}
	\end{center}
\end{figure} 
Abbildung \ref{ableitung:fig:neuronal_network} zeigt die Komponenten eines einfachen neuronalen Netzwerks. $W$ sind die Gewichte, $b$ die Offsets, oft auch \textit{bias} genannt, und $\sigma$ ist die Aktivierungsfunktion. Die Aktivierungsfunktion in den mittleren Schichten wird benötigt, um die Linearität der verschiedenen Schichten zu brechen. In der letzten Schicht wird die Aktivierungsfunktion $\sigma$ aber hauptsächlich verwendet, um den Output auf das vorliegende Problem abzubilden. Eine Ausgabe zwischen $[-\infty, \infty]$ ist erwünscht, sofern ein Regressionsproblem vorliegt. In diesem Fall wird ausnahmsweise eine lineare Funktion für $\sigma$ in der letzten Schicht gewählt. Bei einem stochastischen Problem werden die Outputwerte als Wahrscheinlichkeiten interpretiert und sollten daher den Wertebereich $[0,1]$ annehmen, folglich wird eine Funktion mit entsprechendem Abbildungsbereich für $\sigma$ gewählt. Aus diesem Grund kann $\sigma$ oft eine  sigmoid oder $\tanh$ Funktion und in Ausnahmefällen auch eine lineare Funktion annehmen
\begin{equation}
a^{l}_{j} = \sigma \left( \sum\limits_{k}w^{l}_{jk} \cdot a^{l-1}_{k}+b^{l}_{j} \right)
\label{ableitung:eqn:simple_neuron_eqn}
\end{equation}
beschreibt somit für jedes Neuron $a^{l}_{j}$ aus Abbildung \ref{ableitung:fig:neuronal_network} den Zusammenhang zwischen den Gewichten und den vorhergehenden Neuronen $a^{l-1}_{k}$. Der Input Layer 
\begin{equation}
I_k = a_{k}^{0}
\end{equation}
nimmt die Daten $x_i$ des Datensatzes $x$ entgegen.
Da die Neuronen mit den vorangegangenen Schichten gekoppelt sind, kann man die Gleichung \eqref{ableitung:eqn:simple_neuron_eqn} für die verschiedenen Schichten ineinander einsetzen und abkürzend
\begin{equation}
\hat{y}(x) = \sigma^{l}(W^{l}\sigma^{l-1}(W^{l-1}\cdots \sigma^{1}(W^{1}x+b^{1}) + b^{l-1} \cdots )
\label{ableitung:eqn:full_net}
\end{equation}
schreiben. Dementsprechend resultiert daraus das Minimalproblem
\begin{equation}
	\min \{ \Lb(y, \sigma^{l}(W^{l}\sigma^{l-1}(W^{l-1}\cdots \sigma^{1}(W^{1}x+b^{1}) + b^{l-1} \cdots ) \} \text{,}
\end{equation}
wobei $w$ und $b$ variabel sind und $x$ und $y$ durch den Trainings-Datensatz gegeben sind.
Um die Nomenklatur zusammenzufassen, gelten folgende Variabeln und Parameter in einem neuronalen Netzwerk: 
\begin{itemize}
	\item{$x$: Input ins neuronale Netzwerk}
	\item{$y$: Erwarteter Output aus dem neuronalen Netzwerk gemäss dem Trainingsdatensatz}
	\item{$\hat{y}$: Effektiver gerechneter Output aus den neuronalen Netzwerk}
	\item{$\Lb$: Die Verlustfunktion (\textit{loss}) welche den Fehler zwischen dem gerechenten und dem tatsächlichen Output berechnet.}
	\item{$w_{jk}^{l}$: Das Gewicht des $k$-ten Neurons in der Schicht $(l-1)$ zum $j$-ten Neuron.}
	\item{$b_{j}^{l}$: Das Offset-Gewicht (bias) für das $j$-te Neuron in der Schicht $l$.}
	\item{$\sigma$: Die Aktivierungsfunktion zur Brechnung der Linearität.}
\end{itemize}
Das Trainieren oder Lernen eines neuronalen Netzwerks ist im Grunde nichts anderes als das bestimmen des Minimums des Fehlers mit Hilfe der Paramter $w$ und $b$ für die gegebenen Problemstellung. 