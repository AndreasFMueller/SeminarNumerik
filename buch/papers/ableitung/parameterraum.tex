\subsection{Der Parameterraum}
Das neuronale Netzwerk wird durch eine sehr hohe Anzahl an Parametern
charakterisiert, welche durch den Lernprozesses optimiert werden
müssen.
Der innere Aufbau eines neuronalen Netzwerks erklärt die Herkunft
dieser Parameter.
\begin{figure}
	\begin{center}
		\input{papers/ableitung/neuronal_network.tex}
		\caption{Übersicht eines neuronalen Netzwerks}
		\label{ableitung:fig:neuronal_network}
	\end{center}
\end{figure} 
Abbildung \ref{ableitung:fig:neuronal_network} zeigt die Komponenten eines einfachen neuronalen Netzwerks.
$W$ sind die Gewichte, $b$ die Offsets, oft auch {\em bias} genannt, und $\sigma$ ist die Aktivierungsfunktion.
\index{Gewicht}%
\index{Offset}%
\index{bias}%
Die Aktivierungsfunktion in den mittleren Schichten wird benötigt, um die Linearität der verschiedenen Schichten zu brechen.
\index{Aktivierungsfunktion}%
In der letzten Schicht wird die Aktivierungsfunktion $\sigma$ aber hauptsächlich verwendet, um den Output auf das vorliegende Problem abzubilden.
Eine Ausgabe zwischen $[-\infty, \infty]$ ist erwünscht, sofern ein Regressionsproblem vorliegt.
In diesem Fall ist ausnahmsweise eine lineare Funktion für $\sigma$
in der letzten Schicht erlaubt.
Bei einem stochastischen Problem werden die Outputwerte als
Wahrscheinlichkeiten interpretiert und sollten daher Werte im Intervall
$[0,1]$ annehmen, folglich wird eine Funktion mit entsprechendem
Wertebereich für $\sigma$ gewählt.
Aus diesem Grund kann $\sigma$ oft eine  sigmoid oder $\tanh$
Funktion und in Ausnahmefällen auch eine lineare Funktion sein.
Die Funktion
\begin{equation}
a^{l}_{j} = \sigma \biggl( \sum\limits_{k}w^{l}_{jk} \cdot a^{l-1}_{k}+b^{l}_{j} \biggr)
\label{ableitung:eqn:simple_neuron_eqn}
\end{equation}
beschreibt für jedes Neuron $a^{l}_{j}$ aus Abbildung
\ref{ableitung:fig:neuronal_network} den Zusammenhang zwischen den
Gewichten und den vorhergehenden Neuronen $a^{l-1}_{k}$.
\index{Neuron}%
Der Input Layer 
\begin{equation}
I_k = a_{k}^{0}
\end{equation}
nimmt die Daten $x_i$ des Datensatzes $x$ entgegen.
Da die Neuronen mit den vorangegangenen Schichten gekoppelt sind, kann man die Gleichung \eqref{ableitung:eqn:simple_neuron_eqn} für die verschiedenen Schichten ineinander einsetzen und abkürzend
\begin{equation}
\hat{y}(x) = \sigma^{l}(W^{l}\sigma^{l-1}(W^{l-1}\cdots \sigma^{1}(W^{1}x+b^{1}) + b^{l-1} \cdots )
\label{ableitung:eqn:full_net}
\end{equation}
schreiben. Dementsprechend resultiert daraus das Minimalproblem
\begin{equation}
	\min \{ \Lb(y, \sigma^{l}(W^{l}\sigma^{l-1}(W^{l-1}\cdots \sigma^{1}(W^{1}x+b^{1}) + b^{l-1} \cdots ) \} \text{,} % Klammern ev. ersetzten?
\end{equation}
wobei die Parameter $w$ und $b$ während dem Training bestimmt werden, während $x$ und $y$ durch den Trainings-Datensatz gegeben sind.
Um die Nomenklatur zusammenzufassen, gelten folgende Bezeichnungen in einem neuronalen Netzwerk: 
\begin{itemize}
	\item{$x$: Input ins neuronale Netzwerk}
	\item{$y$: Erwarteter Output aus dem neuronalen Netzwerk gemäss dem Trainingsdatensatz}
	\item{$\hat{y}$: Effektiver gerechneter Output aus den neuronalen Netzwerk}
	\item{$\Lb$: Die Verlustfunktion ({\em loss}) welche den Fehler zwischen dem gerechenten und dem tatsächlichen Output berechnet.}
	\item{$w_{jk}^{l}$: Das Gewicht des $k$-ten Neurons in der Schicht $(l-1)$ zum $j$-ten Neuron.}
	\item{$b_{j}^{l}$: Das Offset-Gewicht ({\em bias}) für das $j$-te Neuron in der Schicht $l$.}
	\item{$\sigma$: Die Aktivierungsfunktion zur Brechung der Linearität.}
\end{itemize}
Das Trainieren oder Lernen eines neuronalen Netzwerks ist im Grunde
nichts anderes als das Bestimmen des Minimums des Fehlers mit Hilfe
der Parameter $w$ und $b$ für die gegebene Problemstellung.
\index{Trainieren}%
\index{Lernen}%
