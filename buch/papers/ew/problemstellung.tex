%
% problemstellung.tex -- Beispiel-File für die Beschreibung des Problems
%
% (c) 2020 Prof Dr Andreas Müller, Hochschule Rapperswil
%
\section{Problemstellung
\label{ew:section:problemstellung}}
\rhead{Problemstellung}


Das Eigenwertproblem sucht diejenigen Vektoren $v_i \in \mathbb{R}^{n} $, die angewendet mit einer Matrix $\bm H$ nicht die Richtung ändern und dabei nur mit $\lambda_i \in \mathbb{R}$ skaliert werden:
\begin{equation}
    \bm H \bm v_i = \lambda_i \bm v_i \label{ew:eq:eig}
\end{equation}

Manche Applikation benötigen nur Eigenwerte und -Vektoren einer Matrix $\bm H(\varepsilon)$, die nur wenig von einer Matrix $\bm H_0$ mit bekannten Eigenwerten $\lambda^{(0)}$ und Eigenvektoren $\bm v^{(0)}$ abweicht.
Dies lässt sich mit der Summe
\begin{equation}
    \bm H(\varepsilon) = \bm H_0 + \varepsilon \bm H_1
\end{equation}
schreiben, wobei $\varepsilon \ll 1 $ ausdrücken soll, dass der zweite Term der Summe viel kleiner ist als $\bm H_0$.




Die Eigenwertperturbation erlaubt es, die Eigenwerte $\lambda_i(\varepsilon)$ und Eigenvektoren  $v_i(\varepsilon)$ von $\bm H$ zu approximieren.
Das Verfahren hat jedoch die Limitierung, dass die Eigenvektoren von $H_0$ zueinander othogonal sind, was für alle symetrischen $\bm H_0$ zutrifft.
Somit fassen wir zusammen:
\begin{aufgabe}
Finde für eine symetrische Matrix $\bm H_0$ mit bekannten Eigenwerten $\lambda_{0}$ und bekannten orthonormierten Eigenvektoren $\bm v_{0i}$ eine approximation erster Ordnung für
\begin{equation*}
    \bm H(\varepsilon) = \bm H_0 + \varepsilon \bm H_1,
\end{equation*}
wobei $\varepsilon \in \mathbb{R^+} \ll 1$.
% \begin{gather*}
%     \bm H(\varepsilon) = \bm H_0 + \varepsilon \bm H_1 \\
%     \varepsilon \in \mathbb{R^+} \ll 1 \\
%     \lambda_{0}, \bm v_{0i} \quad \text{bekannt} \\
%     \bm H_0 \quad \text{symetrisch}
% \end{gather*} %TODO check if only H_0 must be symetric or selbstadjungiert
\end{aufgabe}

\section{Anwendungen}

Die allgemeine Störungstheorie kann überall angewendet werden, wo ein lösbares Problem mit einer kleinen Wechselwirkung gestört wird.
Wie zum Beispiel das Berechnen der Umlaufbahnen von Planeten unter Einfluss von der Anziehungskraft anderer Himmelskörpern, wobei das ungestörteungestörte Zweikörpermodell mit der Keppler-Gleichung gelöst werden kann.  
Ein weiteres Beispiel ist das Berechnen einer Trajektorie unter Berücksichtigung des Luftfwiderstands, wie es in Kapitel \ref{chapter:perturbation} behandelt wird.

Eines der grössten Anwendungsgebiet der Störungstheorie von Eigenwerten ist das Lösen der der Schrödingergleichung in der Quantenmechanik.
Dabei werden die Eigenfunktionen des Hamilton-Operators gesucht um zum Beispiel Spektrallinien von Atomen zu berechnen.
Aus diesem Anwendungsgebiet hat die Theorie auch ihren Ursprung. %TODO check this

\section{Idee}

Die Schreibweise als Funktion von $\varepsilon$ deutet auf eine Taylorreihe hin.
$\bm H(\varepsilon)$ kann als Taylorreihe geschrieben werden.

\begin{equation*}
    \bm H(\varepsilon) = \bm H_0 + \varepsilon \bm H_1 \GR{ + \varepsilon^2 \bm H_2  + \varepsilon^3 \bm H_3 + \dots}
\end{equation*}
Ebenso können die Eigenwerte und Eigenvektoren von $\bm H(\varepsilon)$ als Taylorreihen geschrieben werden.
\begin{align}
    \bm v_i(\varepsilon) = \bm v_{0i} + \varepsilon \bm v_{1i} \GR{ + \varepsilon^2 \bm v_{2i}  + \varepsilon^3 \bm v_{3i} + \dots} \label{ew:eq:eigvec} \\
    \lambda_i(\varepsilon) = \lambda_{0i} + \varepsilon \lambda_{1i} \GR{ + \varepsilon^2 \lambda_{2i}  + \varepsilon^3 \lambda_{3i} + \dots}  \label{ew:eq:eigval}
\end{align}
Für kleine $\varepsilon$ sollte eine Approximation erster Ordnung genügen.
Somit können alle Terme mit $\varepsilon^2$ und höher weggelassen werden.
Falls eine höhere Genauigkeit gewünscht wird, kann auch eine Methode mit einer Approximation zweiter Ordnung gewählt werden.
Diese ist jedoch aufwendiger herzuleiten und wird in diesem Paper nicht angeschaut.

Nach einsetzen der gekürzten Taylorreihen in Gleichung \refeq{ew:eq:eig} erhalten wir
\begin{align}
    \bm H(\varepsilon) \bm v_i(\varepsilon)
    &=
    \lambda_i(\varepsilon) \bm v_i(\varepsilon) \\
    (\bm H_0 + \varepsilon \bm H_1)
    (\bm v_{0i} + \varepsilon \bm v_{1i})
    &=
    (\lambda_{0i} + \varepsilon \lambda_{1i})
    (\bm v_{0i} + \varepsilon \bm v_{1i}),
\end{align}
wobei alle Variablen bekannt sind, ausser $\lambda_{1i}$ und $\bm v_{1i})$.
Die Gleichung soll nun umgeformt werden, um diese variablen zu bestimmen.

\subsection{Herleitung}

Die hier verwendete Herleitung basiert auf \cite{ew:seminar_quantenmechanik}.
Dabei wurde auf die in der Quantenmechanik übliche Bra-Ket-Notation verzichtet.

Als erstes wird die Gleichung ausmultipliziert und alle Terme mit $\varepsilon$ höherer Ordnung als zwei (rot markiert) werden weggelassen.
\begin{equation}
    \GN{\bm H_0 \bm v_{0i}} +
    \varepsilon \bm H_0 \bm v_{1i} +
    \varepsilon \bm H_1 \bm v_{0i} +
    \RD{\varepsilon^2 \bm H_1 \bm v_{1i}}
    =
    \GN{\lambda_{0i} \bm v_{0i}} +
    \varepsilon \lambda_{0i} \bm v_{1i} +
    \varepsilon \lambda_{1i} \bm v_{0i} +
    \RD{\varepsilon^2 \lambda_{1i} \bm v_{1i}}
\end{equation}
Die grün markierten Terme entsprechen genau dem Eigenwertproblem \refeq{ew:eq:eig} und können daher subtrahiert werden. Daraus erhalten wir
\begin{align}
    \varepsilon \bm H_0 \bm v_{1i} +
    \varepsilon \bm H_1 \bm v_{0i}
    &=
    \varepsilon \lambda_{0i} \bm v_{1i} +
    \varepsilon \lambda_{1i} \bm v_{0i}
    \\
    \bm H_0 \bm v_{1i} +
    \bm H_1 \bm v_{0i}
    &=
    \lambda_{0i} \bm v_{1i} +
    \lambda_{1i} \bm v_{0i}
\end{align}
Durch das Linksmultiplizieren mit $\bm v_{0j}^T$ können anschliessend weitere vereinfachungen gemacht werden.
\begin{equation}
    \bm v_{0j}^T \bm H_0 \bm v_{1i} +
    \bm v_{0j}^T \bm H_1 \bm v_{0i}
    =
    \lambda_{0i} \bm v_{0j}^T \bm v_{1i} +
    \lambda_{1i} \bm v_{0j}^T \bm v_{0i}
\end{equation}
Da in den Voraussetzungen orthogonale Eigenvektoren $\bm v_{0i}$ gefordert sind, sind auf der rechten Seite im zweiten Term alle inneren Produkte
\begin{equation}
    \bm v_{0j}^T \bm v_{0i}
    =
    \delta_{ij}
    =
    \begin{cases}
        0 \quad (i \neq j),\\
        1 \quad (i = j)
    \end{cases}.
\end{equation}
%TODO wieso kann selbstadjungiert Trick angewendet werden?
Zusätzlich ist vorausgesetzt, dass $\bm H_0$ symetisch ist.
Dies macht die Matrix auch selbst adjungiert, was bedeutet $ a^T \bm H_0 b =  b^T \bm H_0 a$.
% \begin{equation}
%     \bm v_{0j}^T \bm H_0 \bm v_{1i}
%     =
%     \left( \bm v_{0j}^T \bm H_0 \right) \bm v_{1i}
%     =
%     \bm v_{0j}^T \lambda_{0j} \bm v_{1i}
%     =
%     \lambda_{0j} \bm v_{0j}^T \bm v_{1i}
% \end{equation}
Mithilfe der Eigenwertproblemgleichung können wir dadurch auch den ersten Term auf der linken Seite umformen:
\begin{equation}
    \bm v_{0j}^T \bm H_0 \bm v_{1i}
    =
    \bm v_{1i}^T \left(\bm H_0  \bm v_{0j} \right)
    =
    \bm v_{1i}^T \left(\lambda_{0j}  \bm v_{0j} \right)
    =
    \lambda_{0j} \bm v_{0j}^T \bm v_{1i}
\end{equation}
Daher erhalten wir
\begin{equation}
    \lambda_{0j} \bm v_{0j}^T \bm v_{1i} +
    \bm v_{0j}^T \bm H_1 \bm v_{0i}
    =
    \lambda_{0i} \bm v_{0j}^T \bm v_{1i} +
    \lambda_{1i} \delta_{ij}
\end{equation}
und noch ein wenig umgeformt
\begin{equation}
    \bm v_{0j}^T \bm H_1 \bm v_{0i}
    =
    \delta_{ij} \lambda_{1i} +
    ( \lambda_{0i} - \lambda_{0j} )
    \bm v_{0j}^T  \bm v_{1i} .
\end{equation} \label{ew:eq:f}
Aus dieser Gleichung können mittels Koeffizientenvergleich zwei nützliche Formeln extrahiert werden:
\begin{alignat}{3}
    i = j \quad & \rightarrow  \quad && \lambda_{1i}&& = \bm v_{0i}^T \bm H_1 \bm v_{0i} \\
    i \neq j \quad & \rightarrow \quad && \bm v_{0j}^T \bm v_{1i}&& = \frac{\bm v_{0j}^T \bm H_1 \bm v_{0i}}{\lambda_{0i} - \lambda_{0j}}  \label{ew:eq:f2}
\end{alignat}

\subsection{Nicht entarteter Fall}

Die Eigenwerte $\lambda_{1i}$ können einfach in \refeq{ew:eq:eigval} eingesetzt werden und liefern bereits eine fertige Formel für
\begin{align*}
    \lambda_i(\varepsilon)
    &=
    \lambda_{0i} + \varepsilon \lambda_{1i} \\
    &=
    \lambda_{0i} + \varepsilon \bm v_{0i}^T \bm H_1 \bm v_{0i}
\end{align*}

Die Berechnung der Eigenvektoren ist etwas aufwendiger.
Die Kernidee dabei ist, dass $\bm v_{1i}$ als Summe von Abbildungen auf $\bm v_{0j}$ geschrieben werden kann.
Die Eigenvektoren $\bm v_{1i}$ werden dabei mithilfe von Skalarprodukten $( \bm v_{0j}^T \bm v_{1i})$ in die Eigenbasis $\bm v_{0j}$ konvertiert. %TODO is that true
\begin{align*}
    \bm v_i(\varepsilon)
    &=
    \bm v_{0i} + \varepsilon \bm v_{1i} \\
    &=
    \bm v_{0i} + \varepsilon \sum_{j} ( \bm v_{0j}^T \bm v_{1i}) \, \bm v_{0j}
\end{align*}
Ein graphisches Beispiel dieser Umwandlung ist in Abbildung \ref{ew:fig:scalar_prod} illustriert.
\begin{figure}
    \begin{center}
        \input{papers/ew/tikz/eigenvektor_abbliden.tikz.tex}
    \end{center}
    \caption[Eigenräume]{
        Beispiel einer Darstellung eines Vektors mit Abbildungen auf andere Basis.
        $v_{10}$ ist gleich der Summe der Abbildungen markiert mit rot und blau.
    } \label{ew:fig:scalar_prod}
\end{figure}
Leider stimmt Formel \refeq{ew:eq:f2} nur für $i \neq j$. Damit diese Formel eingesetzt werden kann, muss der Fall $i = j$ aus der Summe genommen werden
\begin{align}
    \bm v_i(\varepsilon)
    &=
    \bm v_{0i} + \varepsilon ( \bm v_{0i}^T \bm v_{1i}) \bm v_{0i} + \varepsilon \sum_{j \neq i} (\bm v_{0j}^T \bm v_{1i}) \, \bm v_{0j} \\
    &=
    \bm v_{0i} ( 1 + (\bm v_{0i}^T \bm v_{1i}) ) + \varepsilon \sum_{j \neq i}
    \frac{\bm v_{0j}^T \bm H_1 \bm v_{0i}}{\lambda_{0i} - \lambda_{0j}}
    \, \bm v_{0j}
\end{align}
\RD{TODO: elaborate $ \mathrm{Im}(\varepsilon \gamma)$}
\begin{equation}
    \bm v_i(\varepsilon)
    =
    \bm v_{0i} ( 1 + \mathrm{Im}(\varepsilon \gamma) ) + \varepsilon \sum_{j \neq i}
    \frac{\bm v_{0j}^T \bm H_1 \bm v_{0i}}{\lambda_{0i} - \lambda_{0j}}
    \, \bm v_{0j}
    \quad
    \QED
    \label{ew:eq:explicit_eigvecs}
\end{equation}

% Zusammenfassend,
% \begin{align*}
%     \lambda_i(\varepsilon)
%     &=
%     \lambda_{0i} + \varepsilon \bm v_{0i}^T \bm H_1 \bm v_{0i}\\
%     \bm v_i(\varepsilon)
%     &=
%     \lambda_{0i} + \varepsilon \bm v_{0i}^T \bm H_1 \bm v_{0i}
%     \bm v_{0i} ( 1 + \mathrm{Im}(\varepsilon \gamma) ) + \varepsilon \sum_{j \neq i}
%     \frac{\bm v_{0j}^T \bm H_1 \bm v_{0i}}{\lambda_{0i} - \lambda_{0j}}
%     \, \bm v_{0j}
% \end{align*}


\section{Entartung}

Die ebenso hergeleitete Formel \refeq{ew:eq:explicit_eigvecs} für die Eigenvektoren ist nicht definiert für einen Spezialfall.
Wenn zwei Eigenwerte von $\bm H_0$ gleich sind, entsteht eine Division durch Null, wie rot hervorgehoben:
\begin{equation*} %TODO maybe ref to equation
    \bm v_i(\varepsilon)
    =
    \bm v_{0i} ( 1+ \mathrm{Im}(\varepsilon \gamma)) + \varepsilon \sum_{j \neq i}
    \frac{\bm v_{0j}^T \bm H_1 \bm v_{0i}}{\RD{\lambda_{0i} - \lambda_{0j}}}
    \, \bm v_{0j}
\end{equation*}
Die gleichen Eigenwerte haben zur Folge, dass die dazugehörigen Eigenvektoren nicht mehr eindeutig definiert sind.
Sie können irgendwo orthogonal auf einer Hyperebene liegen.
Es liegt also kein Eigenvektor mehr vor sondern ein Eigenraum.
Mehrfache Eigenwerte werden auch entartet genannt.
Durch die Störung einer Matrix $\bm H_0$, also durch Addition einer geeigneten Matrix $\varepsilon \bm H_1$, können Eigenwerte gespalten werden werden.
Als Beispiel, wenn $\varepsilon = 0.01$,
\begin{align}
    \bm H_0 &=
    \begin{pmatrix}
        1 & 0 & 0\\
        0 & 2 & 0\\
        0 & 0 & 2
    \end{pmatrix},
    \quad
    \lambda_0 = \{1, 1, 2\},
    \quad
    \bm v_0 = \{
    \begin{pmatrix}
        ?\\
        ?\\
        0
    \end{pmatrix},
    \begin{pmatrix}
        ?\\
        ?\\
        0
    \end{pmatrix},
    \begin{pmatrix}
        0\\
        0\\
        1
    \end{pmatrix}
    \}
    \\
    \bm H_1 &=
    \begin{pmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 2
    \end{pmatrix},
    \quad
    \lambda_0 = \{1, 1, 2\},
    \quad
    \bm v_0 = \{
    \begin{pmatrix}
        1\\
        0\\
        0
    \end{pmatrix},
    \begin{pmatrix}
        0\\
        ?\\
        ?
    \end{pmatrix},
    \begin{pmatrix}
        0\\
        ?\\
        ?
    \end{pmatrix}
    \}
    \\
    \bm H(\varepsilon) &=
    \begin{pmatrix}
        1.01 & 0 & 0\\
        0 & 1.02 & 0\\
        0 & 0 & 2.02
    \end{pmatrix},
    \quad
    \lambda_0 = \{1.01, 1.02, 2.02\},
    \quad
    \bm v_0 = \{
    \begin{pmatrix}
        1\\
        0\\
        0
    \end{pmatrix},
    \begin{pmatrix}
        0\\
        1\\
        0
    \end{pmatrix},
    \begin{pmatrix}
        0\\
        0\\
        1
    \end{pmatrix}
    \}.
\end{align} \label{ew:eq:entartung_bsp}
Die Dimensionen der Eigenvektoren, die einen Eigenraum bilden sind mit Fragezeichen gekennzeichnet.
Die Eigenräume der Matrizen sind in Abbildung \ref{ew:fig:entartung} illustriert.

\begin{figure}
    \begin{center}
        \input{papers/ew/tikz/entartung.tikz.tex}
    \end{center}
    \caption{
        Eigenräume der Beispielmatrizen von \refeq{ew:eq:entartung_bsp}.
        Die mehrdimensionalen Eigenräume sind als Kreise dargestellt, wobei alle orthogonale Vektoren auf der Kreisfläche gültige Eigenvektoren sind.
        Die roten Eigenvektoren ändern schlagartig die richtung, wobei die grünen richtig gewält wurden um die Entartungsrechnung durchzuführen. 
        }
    \label{ew:fig:entartung}
\end{figure}
Die Eigenvektoren ädern sich also schon schlagartig für ganz kleine $\varepsilon = 0.01$.

Bei einer numerischen Berechnung der Eigenvektoren von entarteten Eigenwerten, wie es zum Beispiel MATLAB oder NumPy macht, wird ein Eigenvektor in diesem Eigenraum einfach gewählt.
Dies ist sehr unpraktikabel.
Damit die Störungstheorie auch für entartete Eigenwerte funktioniert, muss das Problem mit der Division durch Null in Formel \refeq{ew:eq:f2} beseitigt werden.
Um dies in den Griff zu kriegen, müssen wir die Formel und Formel \refeq{ew:eq:f} für $i \neq j$ und $\lambda_{0i} = \lambda_{0j}$ anschauen, welche zu dieser Division durch Null geführt hatte.
Dabei erhalten wir
\begin{align}
    \bm v_{0j}^T \bm H_1 \bm v_{0i}
    &=
    \delta_{ij} \lambda_{1i} +
    ( \lambda_{0i} - \lambda_{0j} )
    \bm v_{0j}^T  \bm v_{0i}
    \\
    \bm v_{0j}^T \bm H_1 \bm v_{0i}
    &=
    0 +
    0
    \bm v_{0j}^T  \bm v_{0i}
    = 0.
\end{align}
Falls $i, j$ entartet sind, geht die Gleichung auf.
Wir können also Formel \refeq{ew:eq:explicit_eigvecs} auch für entartete Eigenwerte brauchen, falls
\begin{equation}
    \bm v_{0j}^T \bm H_1 \bm v_{0i} = 0 \quad \forall \quad i,j \quad \text{entartet}.
\end{equation} \label{ew:eq:condition-degenerated}
Im Bruch mit division durch Null haben wir nun $\frac{0}{0}$.
Damit dies stimmt, müssen die entarteten Eigenvektoren $\bm v_{0i}$ so gewählt werden, dass sie in der Eigenbasis von $\bm H_1$ stehen. %TODO elaborate section
Dazu bilden wir $\bm H_1$ auf die Basis von entarteten, zufällig gewählten Eigenvektoren ab
\begin{equation*}
    \bm H^\prime = \bm v_{0i}^T \bm H_1 \bm v_{0i} \quad \forall \quad i \quad \text{entartet}
\end{equation*}
Dadurch erhalten wir eine kleinere Matrix $\bm H^\prime$, welche nur die dimensionen des Eigenraums hat.
Von dieser müssen wir nun die Eigenvektoren finden, died das Eigenwertproblem
\begin{equation*}
    \bm H^\prime \bm v_{i}^\prime = \lambda_{i} \bm v_i^\prime \quad \forall \quad i \quad \text{entartet}
\end{equation*}
bilden.
Allerdings ist dieses Problem einiges einfacher als das ursprüngliche Eigenwertproblem, da die Matrix nur die Entartungen enthält und dadurch viel kleiner ist und die Eigenwerte $\lambda_i$ noch bekannt sind.
Die gefundenen Eigenvektoren müssen nun zurücktransformiert werden und anstelle der ursprünglichen, zufälligen Vektoren verwendet werden.
\begin{equation*}
    \bm v_{0i} \gets \bm v_{0i} \bm v_{i}^\prime \quad \forall \quad i \quad \text{entartet}
\end{equation*}

%TODO wieso genau kann summand weggelassen werden?

Zusammengefasst, auch für entartete $\lambda_0$ gilt somit:
% \begin{aufloesung}
\begin{align*}
    \lambda_i(\varepsilon)
    & \gets
    \lambda_{0i} + \varepsilon \bm v_{0i}^T \bm H_1 \bm v_{0i}\\
    \bm H^\prime & \gets \bm v_{0i}^T \bm H_1 \bm v_{0i} \quad \forall \quad i \quad \text{entartet} \\
    \bm v^\prime & \gets \mathrm{Eig} \Big( \bm H^\prime \Big) \\
    \bm v_{0i} & \gets \bm v_{0i} \bm v^\prime  \quad \forall \quad i \quad \text{entartet} \\
    \bm v_i(\varepsilon)
    & \gets
        \bm v_{0i} ( 1 + \mathrm{Im}(\varepsilon \gamma) ) + \varepsilon \sum_{j \neq i, \text{nicht entartet}}
        \frac{\bm v_{0j}^T \bm H_1 \bm v_{0i}}{\lambda_{0i} - \lambda_{0j}}
        \, \bm v_{0j}.
\end{align*}
% \end{aufloesung}