%
% gausseidel.tex
%
% (c) 2020 Prof Dr Andreas Müller, Hochschule Rapperswil
%
\section{Iterative Gleichungslösung
\label{buch:section:gaussseidel}}
\rhead{Gauss-Seidel-Iteration}
Gegeben ist eine lineares Gleichungssystem von $n$ Gleichungen mit
$n$ Unbekannten, welches wir als
\begin{equation}
\begin{linsys}{5}
a_{11}x_1 &+& a_{12}x_2 &+& \dots  \hspace*{7pt}&+& a_{1n}x_n &=& b_1 \\
a_{21}x_1 &+& a_{22}x_2 &+& \dots  \hspace*{7pt}&+& a_{2n}x_n &=& b_2 \\
\vdots\hspace*{5pt}  & & \vdots\hspace*{5pt}  & & \ddots \hspace*{7pt}& & \vdots\hspace*{5pt}  & & \vdots\hspace*{5pt} \\
a_{21}x_1 &+& a_{n2}x_2 &+& \dots  \hspace*{7pt}&+& a_{nn}x_n &=& b_n
\end{linsys}
\label{buch:eqn:linalg:system}
\end{equation}
Abgekürzt wird das Gleichungssystem auch $Ax=b$ geschrieben, wobei $A=(a_{ij})$
die Koeffizientenmatrix ist, $x=(x_k)$ der Vektor der Unbekannten
und $b=(b_k)$ der Vektor der rechten Seiten.

%
% Iterative Lösung
%
\subsection{Iterative Lösung nach Gauss-Seidel
\label{buch:subsection:gauss-seidel}}
Jede der Gleichungen \eqref{buch:eqn:linalg:system} kann nach Variablen
aufgelöst werden, sofern der zugehörige Koeffizient von $0$ verschieden ist.
Gleichung $k$ in \eqref{buch:eqn:linalg:system} ist
\[
a_{k1}x_1 + a_{k2}x_2 + \dots + a_{kk}x_k + \dots a_{kn}x_n = b_k
\]
Aufgelöst nach $x_k$ ist dies
\[
x_k = \frac1{a_{kk}} (b_k - a_{k1}x_1 - a_{k2}x_2 - \dots - a_{kn}x_n),
\]
sofern $a_{kk}\ne 0$.
Diese Gleichung kann dazu verwendet werden, die Werte für die Unbekannten
zu verbessern.

Wir verwenden daher die Notation $x^{(m)}$ für die $m$-te Approximation
der Lösung.
Mit dieser Notation können wir die Iterations 

\begin{satz}[Gauss-Seidel-Iteration]
Unter geeigneten Voraussetzungen konvergiert die Folge $x^{(m)}$
definiert durch
\begin{equation}
x_k^{(m)}
=
\frac{1}{a_{kk}}\bigl(
b_k  - a_{k1}x_1^{(m)} - \dots - a_{k,k-1}x_{k-1}^{(m)}
- a_{k,k+1}x_{k+1}^{(m-1)} - \dots - a_{kn}x_n^{(m-1)}
\bigr)
\label{buch:eqn:gs:iteration}
\end{equation}
mit Startwert $x^{(0)}=0$
konvergiert gegen die Lösung $x$ des Gleichungssystems $Ax=b$.
\end{satz}

In Abschnitt~\ref{buch:subsection:konvergenzbedingung} werden die
Bedingungen genauer untersucht, die Konvergenz des Verfahrens gegen die
Lösung garantieren können.


\begin{beispiel}
Sei das Gleichungssystem gegeben durch
\begin{equation}
A=\begin{pmatrix}
2&1&1\\
1&3&1\\
1&1&4
\end{pmatrix}
\qquad\text{und}\qquad
b=\begin{pmatrix}
7\\6\\5
\end{pmatrix}.
\label{buch:eqn:gsbeispiel}
\end{equation}
Die Berechnung der Folge $x^{(m)}$ nach
\eqref{buch:eqn:gs:iteration}
liefert die Werte in Tabelle~\ref{buch:table:gaussseidelbeispiel}.
Die Konvergenz scheint linear zu sein.
\begin{table}
\centering
\begin{tabular}{|>{$}r<{$}|>{$}r<{$}>{$}r<{$}>{$}r<{$}|}
\hline
 m & x_1^{(m)} & x_2^{(m)} & x_3^{(m)} \\
\hline
 0 & 0.0000000             & 0.0000000             & 0.0000000             \\
 1 & 3.5000000             & 0.8333333             & 0.1666667             \\
 2 & 3.0000000             & 0.9444444             & 0.2638889             \\
 3 & \underline{2.8}958333 & \underline{0.94}67593 & \underline{0.2}893519 \\
 4 & \underline{2.88}19444 & \underline{0.94}29012 & \underline{0.29}37886 \\
 5 & \underline{2.88}16552 & \underline{0.941}5187 & \underline{0.294}2065 \\
 6 & \underline{2.882}1373 & \underline{0.941}2187 & \underline{0.2941}610 \\
 7 & \underline{2.8823}102 & \underline{0.94117}62 & \underline{0.2941}284 \\
 8 & \underline{2.8823}476 & \underline{0.94117}47 & \underline{0.29411}94 \\
 9 & \underline{2.882353}1 & \underline{0.94117}59 & \underline{0.294117}7 \\
10 & \underline{2.882353}1 & \underline{0.9411764} & \underline{0.2941176} \\
\hline
\infty&   2.8823529 & 0.9411764 & 0.2941176 \\
\hline
\end{tabular}
\caption{Lösung des Gleichungssystems mit Koeffizientenmatrix $A$ und
rechter Seite $b$ aus \eqref{buch:eqn:gsbeispiel} mit Hilfe des
Gauss-Seidel-Algorithmus.
In der letzten Zeile die exakten Resultate, erhalten mit dem
Gauss-Algorithmus.
\label{buch:table:gaussseidelbeispiel}}
\end{table}
\end{beispiel}


%
% Matrixformulierung
%
\subsection{Matrixformulierung
\label{buch:subsection:matrixformulierung}}
Die Iterationsformel~\eqref{buch:eqn:gs:iteration} verknüpft bei der
Berechnung von $x^{(m)}$ Komponenten von $x^{(m-1)}$ und $x^{(m)}$,
was es etwas schwieriger macht, die Iteration als Fixpunktiteration
der Form $x^{(m)} = Fx^{(m-1)}$ zu schreiben mit einer $n\times n$-Matrix $F$.
Um dies zu erreichen zerlegen wir die Matrix $A$ in drei Summanden
$A=L+D+U$, wobei $L$ eine untere Dreiecksmatrix mit Nullen auf der 
Diagonalen sein soll, $D$ eine Diagonalmatrix und $U$ eine obere
Dreiecksmatrix mit  Nullen auf der Diagonalen, also
\begin{align*}
L
&=
\begin{pmatrix}
0        &0        &0        &\dots   & 0         & 0      \\
a_{21}   &0        &0        &\dots   & 0         & 0      \\
a_{31}   &a_{3,2}  &0        &\dots   & 0         & 0      \\
\vdots   &\vdots   &\ddots   &\ddots  & \vdots    & \vdots \\
a_{n-1,1}&a_{n-1,2}&a_{n-1,3}&\dots   & 0         & 0      \\
a_{n1}   &a_{n2}   &a_{n3}   &\dots   & a_{n,n-1} & 0
\end{pmatrix},
\qquad
U
=
\begin{pmatrix}
0      & a_{12} & \dots  & a_{1,n-2} & a_{1,n-1}   & a_{1n} \\
0      & 0      & \dots  & a_{1,n-2} & a_{2,n-1}   & a_{2n} \\
\vdots & \vdots & \ddots & \ddots    &\vdots       & \vdots \\
0      & 0      & \dots  & 0         & a_{n-2,n-1} & a_{n-2,n} \\
0      & 0      & \dots  & 0         & 0           & a_{n-1,n} \\
0      & 0      & \dots  & 0         & 0           & 0
\end{pmatrix}
\intertext{und}
D
&=
\operatorname{diag} (a_{11}, a_{22},\dots , a_{n-1,n-1}, a_{nn}).
\end{align*}
Die Iterationsformel~\eqref{buch:eqn:gs:iteration} lässt sich
mit diesen Matrizen schreiben als
\[
Dx^{(m)} = b - Lx^{(m)} - Ux^{(m-1)}.
\]
Auflösen nach $x^{(m)}$ führt auf
\begin{equation}
x^{(m)} = (D+L)^{-1} ( b - Ux^{(m-1)} ).
\label{buch:eqn:gs:fixpunkt}
\end{equation}
Die Form \eqref{buch:eqn:gs:fixpunkt} für das Gauss-Seidel-Iterationsverfahren
ist jetzt die einer Fixpunkt-Iteration.

%
% Konvergenzbedingung
%
\subsection{Konvergenzbedingung
\label{buch:subsection:konvergenzbedingung}}
In Kapitel~\ref{chapter:berechnung} haben wir gelernt, dass eine
Fixpunktiteration konvergiert, wenn der Betrag der Ableitung $<1$ ist.
Hier liegt jedoch eine Matrix-Iteration mit der Abbildung
\[
F(x)
=
\underbrace{(D+L)^{-1} b}_{\displaystyle=c} - (D+L)^{-1}U x
=
c - (D+L)^{-1}Ux
\]
vor.
Die Ableitung ist daher ebenfalls eine Matrix, nämlich
\[
D_xF = (D+L)^{-1}U,
\]
und der Fehler der Iteration $m$ ist
\begin{equation}
\delta_m = (D+L)^{-1}U \delta_{m-1}.
\label{buch:gs:fehler}
\end{equation}
Konvergenz kann also nur vorliegen, wenn dieser Vektor im Laufe der
Iteration immer kleiner wird.
Dies ist zum Beispiel dann der Fall, wenn die {\em Norm} der Matrix
kleiner als $1$ ist:

\begin{definition}
Die {\em Norm} einer Matrix $M$ ist
\[
\|M\|
=
\max\{|Mx|\,|\, x\in\mathbb R^n\wedge |x|=1\}.
\]
Für einen Vektor $x\in\mathbb R^n$ gilt $|Mx| \le \|M\|\cdot |x|$.
\end{definition}

Die Bedingung \eqref{buch:gs:fehler} bedeutet jedoch nicht,
dass die Norm der Ableitung $<1$ sein muss, es genügt, wenn
genügend hohe Potenzen der Ableitung eine Norm $<1$ haben.

\begin{beispiel}
Die Matrix
\[
M=\begin{pmatrix}
0&2\\
\frac13&0
\end{pmatrix}
\]
hat Norm
\[
\|M\|
=
\max_{|x|=1} |Mx| 
=
\max_{t\in\mathbb R} \sqrt{2^2\cos^2 t +\frac1{3^2}\sin^2t} \ge 2.
\]
Da aber
\[
M^2 = \begin{pmatrix}
\frac{2}{3}&0\\
0&\frac{2}{3}
\end{pmatrix}
\qquad\Rightarrow\qquad \|M^2\|=\frac23
\]
ist, wird eine Iteration mit Ableitungsmatrix $M$ trotzdem
konvergieren, weil der Fehler nach jedem zweiten Schritt um den
Faktor $\frac23$ kleiner geworden ist.
\end{beispiel}

Dies führt uns auf die Grösse
\begin{equation}
\pi(M)
=
\limsup_{n\to\infty} \|M^n\|^\frac1n.
\label{buch:eqn:gelfand-grenzwert}
\end{equation}
Ist $\pi(M) > 1$, dann gibt es Anfangsvektoren $v$ für die Iteration,
für die $M^kv$ über alle Grenzen wächst.
Ist $\pi(M) < 1$, dann wird jeder Anfangsvektor $v$ zu einer Iterationsfolge
$M^kv$ führen, die gegen $0$ konvergiert.
Die Kennzahl $\pi(M)$ erlaubt also zu entscheiden, ob ein
Iterationsverfahren konvergent ist.

Die Berechnung von $\pi(M)$ als Grenzwert ist sehr unhandlich.
Viel einfacher ist der Begriff des Spektralradius.

\begin{definition}
\label{buch:definition:spektralradius}
Der {\em Spektralradius} der Matrix $M$ ist der Betrag des betragsgrössten
Eigenwertes.
\end{definition}

Wir werden später zeigen, dass $\pi(M) = \varrho(M)$, dies ist auch
bekannt als der Satz von Gelfand.
Um die beiden Begriffe bis dann auseinander halten zu können,
nennen wir den Grenzwert~\ref{buch:eqn:gelfand-grenzwert}
den {\em Gelfand-Radius} $\pi(M)$ der Matrix $M$.
Wir erlauben uns aber, die Überlegungen zu den Iterationsverfahren
mit dem Spektralradius zu formulieren.

Das Gauss-Seidel-Iterationsverfahren ist also genau dann für alle
Startwerte $x_0$ linear konvergent, wenn der Spektralradius
\[
\varrho\bigl( (L+D)^{-1}U \bigr) < 1
\]
ist.

\subsection{Zerlegungsverfahren
\label{buch:subsection:zerlegung}}
Das Gauss-Seidel-Verfahren ist nur ein Beispiel einer ganzen Familie
von iterativen Lösungsverfahren für lineare Gleichungssysteme.
Sie basieren alle auf einer Zerlegung $A=B+C$ der Matrix $A$.
Das Gauss-Seidel-Verfahren ist der Fall
\[
A = \underbrace{D+L}_{\displaystyle = B} + \underbrace{R}_{\displaystyle = C}.
\]
Das ursprüngliche Gleichungssystem $Ax=b$ wird jetzt ebenfalls
aufgeteilt in $Bx+Cx=b$.
Ein iteratives Verfahren ergibt sich jetzt dadurch, dass für die beiden $x$
in der Aufteilung verschiedene Iterationen der Lösung verwendet werden,
also $Bx^{(m+1)} + Cx^{(m)} = b$.
Dies ist natürlich nur sinnvoll, wenn sich die Matrix $B$ wesentlich
leichter invertieren lässt als $A$, da andernfalls die Bestimmung von
$x^{(m+1)}$ nicht einfacher ist als die Lösung des ursprünglichen
Gleichungssystems.
Wir erhalten somit das Iterationsverfahren
\begin{equation}
x^{(m+1)}
=
B^{-1}(b-Cx^{(m)})
=
b_0 - B^{-1}Cx^{(m)}
\end{equation}

Iteriert wird also die Anwendung der Matrix $B^{-1}C$.
Aus der oben entwickelten Theorie lesen wir ab, dass das Zerlgungsverfahren
genau dann konvergiert, wenn der Spektralradius 
$\varrho(B^{-1}C)$ von $B^{-1}C$ kleiner ist als $1$.

\subsubsection{Das Verfahren von Jacobi}
Beim Gauss-Seidel-Verfahren wurde jede einzelne Gleichung des
Gleichungssytems nach einer der Variablen aufgelöst und der neue
Wert bei der nächsten Iteration gleich wieder verwendet.
Man hätte natürlich auch erst aus jeder Gleichung einen neuen
Wert für alle Variablen bestimmen können, bevor man diese neuen
Werte in der nächsten Iteration verwendet.
Schreibt man das Gleichungssysten wieder als
\[
(L+D+U) x
=
Lx + D {\color{red}x} + Ux
=
b
\]
dann bedeutet dies, dass man  nach der roten Variablen ${\color{red}x}$
auflöst, also
\[
D{\color{red}x}
=
Lx + Ux + b
\qquad\Rightarrow\qquad
{\color{red}x}
=
D^{-1}(L+U)x + D^{-1}b.
\]
Auch dieses nach {\em Jacobi} benannte Verfahren ist also ein
Zerlegungsverfahren, diesmal mit $B=D$ und $C=L+U$.
Es konvergiert genau dann, wenn $\varrho(D^{-1}(L+U))<1$.
Dieser Fall tritt dann ein, wenn die Diagonalelemente  sehr viel grösser
sind als der Rest der Matrix.

\subsubsection{Richardson-Verfahren}
Das {\em Richardson-Verfahren} ist besonders gut geeignet für den
Fall, dass die Matrix $A$ nahe an einem Vielfachen der Einheitsmatrix
ist.
Man verwendet dazu die Aufspaltung
\[
A = B + C = \tau E  + (A - \tau E).
\]
Die Matrix $B=\tau E$ ist sehr einfach zu invertieren: $B^{-1}=\frac1\tau E$.
Das Richardson-Verfahren zeichnet sich also durch sehr geringen Aufwand
bei der Invertierung aus.
Die zu iterierende Matrix ist 
\[
B^{-1}C
=
\frac{1}{\tau}E(A-\tau E)
=
\frac1\tau A  - E.
\]
Je näher die Matrix $A$ bei $\tau E$ liegt, desto näher werden die 
Eigenwerte von $A$ bei denen von $\tau E$ also bei $\tau$ liegen,
und desto näher werden die Eigenwerte von $B^{-1}C$ bei $0$ liegen,
was zu Konvergenz des Verfahrens führt.

\subsubsection{Successive Overrelaxation (SOR)}
Das Richardson-Verfahren enthält einen Parameter, mit dem man die Konvergenz
beeinflussen kann.
Das Gauss-Seidel-Verfahren andererseits versucht jede einzelne Variable
ohne Rücksicht auf alle anderen zu korrigieren.
Die einzelne Korrektur soll also den ganzen verbleibenden Fehler
ausbügeln.
Es ist verständlich, dass damit zu viel korrigiert wird.
Man kann diese Überkorrektur verbessern und einen Parameter einführen.
Man nennt dies {\em Successive Overrelaxation} und verwendet dafür die
Aufspaltung
\[
A = B_\omega + C_\omega
=
\underbrace{
\biggl(
\frac1{\omega}D+L
\biggr)}_{\displaystyle=B_\omega}
+
\underbrace{
\biggl(1-\frac1\omega\biggr) D+L}_{\displaystyle = C_\omega}.
\]
Geignete Werte von $\omega$ liegen zwischen $0$ und $2$,
das Gauss-Seidel-Verfahren ist der Fall $\omega = 1$.
Man kann zeigen, dass dieses Verfahren für symmetrische Matrizen
immer konvergiert.

\begin{beispiel}
Zur Illustration berechnen wir die verschiedenen Spektralradien für
eine symmetrische $30\times 30$-Matrix der Form $E + R$,
wobei $R$ aus zufälligen Werten
im Interval $[0,0.1]$ besteht.
\begin{figure}
\centering
\includegraphics{chapters/60-linsys/images/sp.pdf}
\caption{Spektralradius für das Richardson-Verfahren in Abhängigkeit
von $\tau$ und für SOR in Abhängigkeit von $\omega$.
In beiden Fällen gibt es einen Parameterwert, für den die
Konvergenzgeschwindigkeit maximal ist.
\label{buch:figure:spektralradius}}
\end{figure}
In Abbildung ist der Spektralradius der Iterationsmatrix für
das Richardson-Verfahren und für SOR dargestellt.
In beiden Fällen gibt es einen Wert für den Parameter,
für den der Spektralradius minimal und damit die Konvergenzgeschwindigkeit
am grössten wird.
\end{beispiel}
